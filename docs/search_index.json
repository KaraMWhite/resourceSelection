[["index.html", "Plains spotted skunks in South Dakota Overview", " Plains spotted skunks in South Dakota Kara White 2023-04-25 Overview The plains spotted skunk is a small (i.e., ≤1 kg) carnivore native to central North America with a historical distribution extending from the southern plains of Manitoba, Canada to the coastal plains of Tamaulipas, Mexico, and east-west from the Mississippi River to the Rocky Mountains. Figure 0.1: photo by Danni Brosend Long-term harvest trends of spotted skunks east of the Rocky Mountains indicated a &gt;90% reduction in populations. Causes for the reduction have not been identified, but intensified land-use practices that reduce habitat via agricultural expansion, decrease prey abundances via widespread use of agricultural pesticides, and increase competition via altered predator communities have been hypothesized. Perceived population concerns led to a petition to consider plains spotted skunks under the U.S. Endangered Species Act and the United States Fish and Wildlife Service (USFWS) conducted a species status assessment of the plains spotted skunk. However, there is a paucity of information about plains spotted skunks to inform conservation decisions and efforts. Few studies have investigated plains spotted skunks, and even fewer have focused on the northern portion of the species range. Follow along with me as I explore spotted skunk data in partial fulfillment of the requirements of RDS 2023 course, and as I attempt to address knowledge gaps and inform conservation and management of plains spotted skunks in South Dakota! "],["relational-database.html", "Chapter 1 Relational Database 1.1 Designing a Database 1.2 Creating the Database 1.3 Creating tables 1.4 Reading in CSVs 1.5 Populating database tables 1.6 Querying the database", " Chapter 1 Relational Database This chapter documents the creation of a relational database in partial fulfillment of the RDS 2023 course requirements. 1.1 Designing a Database Below is an image of the database structure I will to use to host the data for my final semester project. The diagram describes the tables that will compose my database. Primary keys are in bold and foreign keys are in italics. 1.2 Creating the Database My first step is to create a database and establish a connection between R and SQLite library(DBI) #load package skunks_db &lt;- dbConnect(RSQLite::SQLite(), &quot;C:/Users/Kara.White/Documents/PhD/resourceSelection/data/skunks.db&quot;) #establish db connection 1.3 Creating tables Now that I have created and established a connection to the database I can begin to create tables #create skunk table dbExecute(skunks_db, &quot;CREATE TABLE skunks ( skunk_id varchar(10) NOT NULL, sex char(1) CHECK (sex IN (&#39;M&#39;, &#39;F&#39;)), age_class varchar(8) CHECK (age_class IN (&#39;juvenile&#39;, &#39;subadult&#39;, &#39;adult&#39;)), PRIMARY KEY (skunk_id) );&quot;) #create capture sites table dbExecute(skunks_db, &quot;CREATE TABLE capture_sites ( site varchar(7) NOT NULL PRIMARY KEY, utm_x double, utm_y double );&quot;) #create captures table dbExecute(skunks_db, &quot;CREATE TABLE captures ( capture_id INTEGER PRIMARY KEY AUTOINCREMENT, skunk_id varchar(10), date text, site varchar(7), FOREIGN KEY(skunk_id) REFERENCES skunks(skunks_id) FOREIGN KEY(site) REFERENCES capture_sites(site) );&quot;) #create biometrics table dbExecute(skunks_db, &quot;CREATE TABLE biometrics ( measurement_id INTEGER PRIMARY KEY AUTOINCREMENT, skunk_id varchar(8), date text, reproductive_status char(20), weight_g float, ear_nose_mm float, left_ear_mm float, right_hindfoot_mm float, head_mm float, body_mm float, tail_mm float, neck_mm float, temperature_F float, pulse_per_min float, respiration_per_min float, eye_condition char(10), teeth_condition varchar(10), FOREIGN KEY (skunk_id) REFERENCES skunks(skunk_id) );&quot;) #create biological samples table dbExecute(skunks_db, &quot;CREATE TABLE biological_samples ( sample_id INTEGER PRIMARY KEY AUTOINCREMENT, skunk_id varchar(8), date text, hair char(1) CHECK (hair IN (&#39;Y&#39;, &#39;N&#39;)), ear_snip char(1) CHECK (ear_snip IN (&#39;Y&#39;, &#39;N&#39;)), scat char(1) CHECK (scat IN (&#39;Y&#39;, &#39;N&#39;)), parasites char(1) CHECK (parasites IN (&#39;Y&#39;, &#39;N&#39;)), FOREIGN KEY (skunk_id) REFERENCES skunks(skunk_id) );&quot;) #create tags table dbExecute(skunks_db, &quot;CREATE TABLE tags ( tag_id INTEGER PRIMARY KEY AUTOINCREMENT, skunk_id varchar(8), ear_left char(3), ear_right char(3), collar_id varchar(10), collar_type char(3), FOREIGN KEY (skunk_id) REFERENCES skunks(skunk_id) );&quot;) #create deployments table dbExecute(skunks_db, &quot;CREATE TABLE deployments ( deployment_id INTEGER PRIMARY KEY AUTOINCREMENT, skunk_id varchar(8), collar_id varchar(10), start_date text, end_date text, FOREIGN KEY (skunk_id) REFERENCES skunks(skunk_id) FOREIGN KEY (collar_id) REFERENCES tags(collar_id) );&quot;) #create gps_data_raw table dbExecute(skunks_db, &quot;CREATE TABLE gps_data_raw ( gps_id INTEGER PRIMARY KEY AUTOINCREMENT, collar_id varchar(10), date text, time_24hr text, utm_x double, utm_y double, hdop double, FOREIGN KEY(collar_id) REFERENCES tags(collar_id) );&quot;) #create vhf_data_raw table dbExecute(skunks_db, &quot;CREATE TABLE vhf_data_raw ( vhf_id INTEGER PRIMARY KEY AUTOINCREMENT, collar_id varchar(10), date text, time_24hr text, utm_x double, utm_y double, error_ellipse_m2 double, FOREIGN KEY(collar_id) REFERENCES tags(collar_id) );&quot;) #create location data table dbExecute(skunks_db, &quot;CREATE TABLE location_data ( loc_id INTEGER PRIMARY KEY AUTOINCREMENT, skunk_id varchar(8), collar_id varchar(10), date text, time_24hr text, utm_x double, utm_y double, FOREIGN KEY (skunk_id) REFERENCES skunks(skunk_id) FOREIGN KEY(collar_id) REFERENCES tags(collar_id) );&quot;) # create spotted skunk observation table dbExecute(skunks_db, &quot;Create Table observations ( obs_id INTEGER PRIMARY KEY AUTOINCREMENT, Species char(9), Year text(4), County varchar(15), Latitude double, Longitude double, Source varchar(20) ) &quot;) 1.4 Reading in CSVs Now that I have finished creating all of the tables as shown in 1.1 I can load the data that I will use to populate the tables skunks &lt;- read.csv(&quot;~/PhD/resourceSelection/data/skunks.csv&quot;, stringsAsFactors = F) capture_sites &lt;- read.csv(&quot;~/PhD/resourceSelection/data/capture_sites.csv&quot;, stringsAsFactors = F) captures &lt;- read.csv(&quot;~/PhD/resourceSelection/data/captures.csv&quot;, stringsAsFactors = F) biometrics &lt;- read.csv(&quot;~/PhD/resourceSelection/data/Biometrics.csv&quot;, stringsAsFactors = F) biological_samples &lt;- read.csv(&quot;~/PhD/resourceSelection/data/samples.csv&quot;, stringsAsFactors = F) tags &lt;- read.csv(&quot;~/PhD/resourceSelection/data/tags.csv&quot;, stringsAsFactors = F) deployments &lt;- read.csv(&quot;~/PhD/resourceSelection/data/deployments.csv&quot;, stringsAsFactors = F) observations &lt;- read.csv(&quot;~/PhD/resourceSelection/website/SPPU_SDLocations.csv&quot;, stringsAsFactors = F) Before I populate the tables I will check to make sure column names match the ones in the tables I created and rearrange columns so that they appear in the right order names(skunks) ## [1] &quot;skunk_id&quot; &quot;sex&quot; &quot;age_class&quot; names(capture_sites) ## [1] &quot;site&quot; &quot;utm_x&quot; &quot;utm_y&quot; The first two CSVs have columns with correct names and in the correct order, but I will need to add auto-incremental columns and then re-order the columns for the remaining CSVs. library(dplyr) captures$capture_id &lt;- 1:nrow(captures) #add column head(captures, 2) #check column added ## skunk_id date site capture_id ## 1 01-20-F 4/18/2020 F006-20 1 ## 2 02-20-M 4/19/2020 F003-20 2 captures &lt;- captures[, c(&quot;capture_id&quot;, &quot;skunk_id&quot;, &quot;date&quot;, &quot;site&quot;)] #reorder columns biometrics$measurement_id &lt;- 1:nrow(biometrics) #add column head(biometrics, 2) #check column added ## skunk_id date reproductive_status weight_g ear_nose_mm left_ear_mm ## 1 02-20-M 4/20/2022 Distended testes 800 27 20 ## 2 01-20-F 4/19/2022 Not active 630 24 27 ## right_hindfoot_mm head_mm body_mm tail_mm neck_mm temperature_F pulse_per_min ## 1 44 62 285 218 NA 95.9 68 ## 2 40 72 252 192 1200 97.4 120 ## respiration_per_min eye_condition teeth_condition measurement_id ## 1 12 Normal Worn 1 ## 2 NA Normal Good 2 biometrics &lt;- biometrics %&gt;% select(measurement_id, everything()) #reorder columns head(biometrics, 2) #check columns are re-ordered ## measurement_id skunk_id date reproductive_status weight_g ear_nose_mm ## 1 1 02-20-M 4/20/2022 Distended testes 800 27 ## 2 2 01-20-F 4/19/2022 Not active 630 24 ## left_ear_mm right_hindfoot_mm head_mm body_mm tail_mm neck_mm temperature_F ## 1 20 44 62 285 218 NA 95.9 ## 2 27 40 72 252 192 1200 97.4 ## pulse_per_min respiration_per_min eye_condition teeth_condition ## 1 68 12 Normal Worn ## 2 120 NA Normal Good biological_samples$sample_id &lt;- 1:nrow(biological_samples) #add column head(biological_samples, 2) #check column added ## skunk_id date hair ear_snip scat parasites sample_id ## 1 01-20-F 4/19/2020 Y Y N N 1 ## 2 02-20-M 4/20/2020 Y Y N N 2 biological_samples &lt;- biological_samples %&gt;% select(sample_id, everything()) #reorder columns head(biological_samples, 2) #check columns are re-ordered ## sample_id skunk_id date hair ear_snip scat parasites ## 1 1 01-20-F 4/19/2020 Y Y N N ## 2 2 02-20-M 4/20/2020 Y Y N N tags$tag_id &lt;- 1:nrow(tags) head(tags, 2) #check column added ## skunk_id ear_right ear_left collar_id collar_type tag_id ## 1 01-22-M 30 41 165.252 VHF 1 ## 2 02-22-F 96 68 165.371 VHF 2 tags &lt;- tags %&gt;% select (tag_id, everything()) #reorder columns head(tags, 2) #check columns are re-ordered ## tag_id skunk_id ear_right ear_left collar_id collar_type ## 1 1 01-22-M 30 41 165.252 VHF ## 2 2 02-22-F 96 68 165.371 VHF deployments$deployment_id &lt;- 1:nrow(deployments) head(deployments, 2) #check column added ## skunk_id collar_id start_date end_date deployment_id ## 1 01-22-M 165.252 3/29/2022 8/12/2022 1 ## 2 02-22-F 165.371 3/30/2022 8/12/2022 2 deployments &lt;- deployments %&gt;% select (deployment_id, everything()) #reorder columns head(deployments, 2) #check columns are re-ordered ## deployment_id skunk_id collar_id start_date end_date ## 1 1 01-22-M 165.252 3/29/2022 8/12/2022 ## 2 2 02-22-F 165.371 3/30/2022 8/12/2022 observations$obs_id &lt;- 1:nrow(observations) # add column head(observations, 2) ## Species Year County Latitude Longitude Source obs_id ## 1 Spilogale 1954 Clay 42.82260 -96.9141 Database 1 ## 2 Spilogale 1961 Lyman 43.69736 -100.0405 Database 2 observations &lt;- observations %&gt;% select (obs_id, everything()) #reorder columns 1.5 Populating database tables Now that I have checked that all column names match the column names I assigned when creating the tables I can insert the data into the tables #add data from csv into captures table dbWriteTable(skunks_db, &quot;skunks&quot;, skunks, append = T) #add data from csv into capture_sites table dbWriteTable(skunks_db, &quot;capture_sites&quot;, capture_sites, append = T) #add data from csv into captures table dbWriteTable(skunks_db, &quot;captures&quot;, captures, append = T) #add data from csv into biometrics table dbWriteTable(skunks_db, &quot;biometrics&quot;, biometrics, append = T) #add data from csv into biological samples table dbWriteTable(skunks_db, &quot;biological_samples&quot;, biological_samples, append = T) #add data from csv into biological samples table dbWriteTable(skunks_db, &quot;tags&quot;, tags, append = T) #add data from csv into biological samples table dbWriteTable(skunks_db, &quot;deployments&quot;, deployments, append = T) #add data from csv into reports table dbWriteTable(skunks_db, &quot;observations&quot;, observations, overwrite = T) 1.6 Querying the database Now that I have inserted data into the table, I can make basic queries to check that the data has been inserted into the tables correctly dbGetQuery(skunks_db, &quot;SELECT * FROM skunks LIMIT 10;&quot;) #skunks table ## skunk_id sex age_class ## 1 01-20-F F adult ## 2 02-20-M M adult ## 3 01-21-M M adult ## 4 02-21-M M adult ## 5 03-21-F F adult ## 6 04-21-M M adult ## 7 05-21-M M adult ## 8 06-21-F F adult ## 9 07-21-M M adult ## 10 08-21-M M adult dbGetQuery(skunks_db, &quot;SELECT * FROM capture_sites LIMIT 10;&quot;) #capture sites table ## site utm_x utm_y ## 1 F003-20 481063 4987647 ## 2 F006-20 480943 4989279 ## 3 F012-20 478896 4988372 ## 4 F016-21 478920 4988815 ## 5 F033-21 477448 4994177 ## 6 F034-21 478153 4994150 ## 7 F043-21 478135 4994097 ## 8 F048-21 480580 4998646 ## 9 F053-21 478288 4998991 ## 10 F066-21 480551 4995177 dbGetQuery(skunks_db, &quot;SELECT * FROM captures LIMIT 10;&quot;) #captures table ## capture_id skunk_id date site ## 1 1 01-20-F 4/18/2020 F006-20 ## 2 2 02-20-M 4/19/2020 F003-20 ## 3 3 &lt;NA&gt; 4/19/2020 F012-20 ## 4 4 01-21-M 3/20/2021 F033-21 ## 5 5 02-21-M 3/23/2021 F034-21 ## 6 6 01-21-M 3/25/2021 F034-21 ## 7 7 03-21-F 3/25/2021 F043-21 ## 8 8 04-21-M 3/27/2021 F033-21 ## 9 9 04-21-M 4/2/2021 F034-21 ## 10 10 05-21-M 4/3/2021 F034-21 dbGetQuery(skunks_db, &quot;SELECT * FROM biometrics LIMIT 10;&quot;) #biometrics table ## measurement_id skunk_id date reproductive_status weight_g ear_nose_mm ## 1 1 02-20-M 4/20/2022 Distended testes 800 27.0 ## 2 2 01-20-F 4/19/2022 Not active 630 24.0 ## 3 3 01-21-M 3/20/2021 Distended testes 950 24.0 ## 4 4 01-21-M 3/25/2021 Distended testes 950 NA ## 5 5 02-21-M 3/23/2021 Distended testes 800 20.5 ## 6 6 03-21-F 3/25/2021 Distended testes 650 25.0 ## 7 7 03-21-F 5/6/2021 Not active 525 24.0 ## 8 8 04-21-M 3/27/2021 Not active 975 22.0 ## 9 9 05-21-M 4/3/2021 Distended testes 800 26.0 ## 10 10 06-21-F 4/3/2021 Distended testes 650 20.0 ## left_ear_mm right_hindfoot_mm head_mm body_mm tail_mm neck_mm temperature_F ## 1 20 44 62 285 218 NA 95.9 ## 2 27 40 72 252 192 1200 97.4 ## 3 21 50 85 295 210 NA NA ## 4 NA NA NA NA NA NA 97.6 ## 5 30 48 83 260 230 120 98.6 ## 6 23 42 74 220 140 100 96.3 ## 7 NA 42 104 220 185 90 92.3 ## 8 28 49 75 290 210 NA 95.4 ## 9 26 45 85 280 200 120 94.8 ## 10 NA 41 72 NA 185 110 96.0 ## pulse_per_min respiration_per_min eye_condition teeth_condition ## 1 68 12 Normal Worn ## 2 120 NA Normal Good ## 3 68 80 Normal Good ## 4 NA NA Normal Good ## 5 52 48 Normal Good ## 6 72 84 Normal Good ## 7 NA 44 Normal Good ## 8 NA 76 Normal Broken M1 ## 9 52 40 Normal Good ## 10 40 36 Normal Good dbGetQuery(skunks_db, &quot;SELECT * FROM biological_samples LIMIT 10;&quot;) #samples table ## sample_id skunk_id date hair ear_snip scat parasites ## 1 1 01-20-F 4/19/2020 Y Y N N ## 2 2 02-20-M 4/20/2020 Y Y N N ## 3 3 01-21-M 3/20/2021 Y Y N N ## 4 4 01-21-M 3/25/2021 N Y Y N ## 5 5 02-21-M 3/23/2021 Y Y N Y ## 6 6 03-21-F 3/25/2021 Y Y N Y ## 7 7 03-21-F 5/6/2021 N N N N ## 8 8 04-21-M 3/27/2021 Y Y N N ## 9 9 05-21-M 4/3/2021 Y Y Y Y ## 10 10 06-21-F 4/3/2021 Y Y N N dbGetQuery(skunks_db, &quot;SELECT * FROM tags LIMIT 10;&quot;) #tags table ## tag_id skunk_id ear_left ear_right collar_id collar_type ## 1 1 01-22-M 41 30 165.252 VHF ## 2 2 02-22-F 68 96 165.371 VHF ## 3 3 03-22-M 39 95 165.4 VHF ## 4 4 04-22-M 34 94 165.303 VHF ## 5 5 05-22-M 37 78 165.27 VHF ## 6 6 06-22-F 93 54 165.21 GPS ## 7 7 07-22-F 68 31 165.421 VHF ## 8 8 01-21-M 76 100 165.064 VHF ## 9 9 02-21-M 23 1 165.2 GPS ## 10 10 03-21-F 22 2 165.022 VHF dbGetQuery(skunks_db, &quot;SELECT * FROM deployments LIMIT 10;&quot;) #deployments table ## deployment_id skunk_id collar_id start_date end_date ## 1 1 01-22-M 165.252 3/29/2022 8/12/2022 ## 2 2 02-22-F 165.371 3/30/2022 8/12/2022 ## 3 3 03-22-M 165.4 4/3/2022 8/12/2022 ## 4 4 04-22-M 165.303 4/10/2022 8/12/2022 ## 5 5 05-22-M 165.27 4/24/2022 8/12/2022 ## 6 6 06-22-F 165.21 5/3/2022 8/12/2022 ## 7 7 07-22-F 165.421 5/25/2022 8/12/2022 ## 8 8 01-21-M 165.064 3/20/2021 7/14/2021 ## 9 9 02-21-M 165.2 3/25/2021 4/1/20221 ## 10 10 03-21-F 165.022 3/25/2021 5/6/2021 ## report_id Species Year County Latitude Longitude Source ## 1 1 Spilogale 1954 Clay 42.82260 -96.91410 VertNet ## 2 2 Spilogale 1961 Lyman 43.69736 -100.04046 VertNet ## 3 3 Spilogale 1965 Sanborn 44.06667 -98.26944 SDNHD ## 4 4 Spilogale 1965 Hamlin 44.75970 -97.37920 VertNet ## 5 5 Spilogale 1965 Roberts 45.40225 -96.66054 VertNet ## 6 6 Spilogale 1970 Bon Homme 43.13414 -97.70280 Trapper ## 7 7 Spilogale 1972 Meade 45.02083 -102.18889 SDNHD ## 8 8 Spilogale 1975 Potter 45.07187 -99.77059 ListServ ## 9 9 Spilogale 1975 McCook 43.68697 -97.41057 Trapper ## 10 10 Spilogale 1981 Lyman 43.79861 -99.68194 SDNHD "],["plains-spotted-skunk-distribution-in-south-dakota.html", "Chapter 2 Plains Spotted Skunk Distribution in South Dakota 2.1 Spotted skunk locations across the state 2.2 Sources of reported spotted skunk observations 2.3 Contemporary Data and geographical uncertainty 2.4 Comparisons of spatial extents and presence:pseudo-absence ratios 2.5 Climate data exploration 2.6 Correlations between predictor variables 2.7 Next steps", " Chapter 2 Plains Spotted Skunk Distribution in South Dakota Figure 2.1: A ‘wanted’ flyer created to solicit spotted skunk observations I conducted a widespread campaign to solicit plains spotted skunk observations in South Dakota. I encouraged respondents to report plains spotted skunk observations via phone, email, or online through an iNaturalist project page that we created to aggregate online reports. Targeted respondents included state natural resource managers, academic institutions, furbearer license holders and state trapper association members, agricultural landowners, and members of various hunting and wildlife-oriented social groups. Information appeared in multiple news publications throughout the state, including in print, online, and over the radio. I also queried the VertNet and Global Biodiversity Information Facility (GBIF) databases, reviewed published and ongoing wildlife research in South Dakota, and accessed data maintained by the South Dakota Game Fish and Parks (SDGFP) Natural Heritage Program. Finally, I reviewed information from the South Dakota Department of Transportation’s roadkill image database for all roadkill tagged as a “small animal.” I compiled all observations into a table I named “obs”. How many observations did I collect? obs %&gt;% summarise(obs_id = n()) ## obs_id ## 1 186 I collected 186 records. Let’s see how those records vary by county. 2.1 Spotted skunk locations across the state ## Reading layer `County_Boundaries__Census_2010&#39; from data source ## `C:\\Users\\Kara.White\\OneDrive - South Dakota State University - SDSU\\Dissertation - SDSU\\SDHM\\sdhmR-V2022.1\\spottedskunk\\SDCountyBoundaries&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 66 features and 37 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -104.0577 ymin: 42.47969 xmax: -96.43648 ymax: 45.94572 ## Geodetic CRS: WGS 84 SDak %&gt;% st_join(obs_sf) %&gt;% group_by(NAME) %&gt;% summarize(n_county = n()) %&gt;% ggplot() + geom_sf(aes(fill = n_county)) + scale_fill_viridis_c(name = &quot;Number of observations&quot;, option = &quot;magma&quot;) + labs(title = &quot;Spotted skunk occurrences by county&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) We can see that most reported spotted skunk observations were clustered in eastern South Dakota, and that one county had over 50 observations! That’s because one trapper provided a lot of data. 2.2 Sources of reported spotted skunk observations Let’s take a look at some of the data providers obs %&gt;% count(Source) %&gt;% mutate(Source = reorder(Source, n)) %&gt;% ggplot(aes(x = Source, y = n, fill = Source)) + geom_bar(stat = &#39;identity&#39;) + coord_flip() + labs(title = &quot;Reported spotted skunk observations by source&quot;, x = &quot;&quot;, y = &quot;Count&quot;, fill = &quot;Source&quot;) + guides(fill = FALSE) + theme_light() We can see that trappers provides the majority of spotted skunk observations, followed by databases, social media, and prior research. 2.3 Contemporary Data and geographical uncertainty Some of those records were historical and do not match contemporary predictor data which is available from 1970. How many reports do I have that occurred before 1970? obs %&gt;% tally(Year &lt;= 1970) ## n ## 1 6 I have 6 records before 1970, which I want to omit before analyses. We also had some locational uncertainty because some trappers provided locations to the nearest 1/4 section of a particular township and range rather than specific coordinates. I will randomly exclude one location from a pair of records that occurred closer than the estimated locational threshold to account for potential duplication of observations and to reduce the effects of spatial autocorrelation. obs_thin &lt;- obs %&gt;% filter(Year &gt;= 1970 &amp; Year &lt;= 2022) %&gt;% spThin::thin(lat.col = &quot;Latitude&quot;, long.col = &quot;Longitude&quot;, spec.col = &quot;Species&quot;, thin.par = 3, reps = 1, locs.thinned.list.return = T, write.files = F) ## ********************************************** ## Beginning Spatial Thinning. ## Script Started at: Tue Apr 25 11:02:21 2023 ## lat.long.thin.count ## 129 ## 1 ## [1] &quot;Maximum number of records after thinning: 129&quot; ## [1] &quot;Number of data.frames with max records: 1&quot; ## [1] &quot;No files written for this run.&quot; After spatial thinning we have 129 spotted skunk observations that I will use for analyses. 2.4 Comparisons of spatial extents and presence:pseudo-absence ratios Most of the spotted skunk observations are in eastern South Dakota. Should I should omit the far western points and concentrate my analyses on the eastern part of the state, or retain a statewide analysis? Furthermore, while species distribution modeling require presence-only datasets, Elith et al. (2006) found that presence-absence models performed better than presence-only models, so I want to incorporate pseudo-absences (PsA) into my dataset. My next step is to generate pseudo-absence (PsA) locations, but there is no general consensus on how many to use. Well, since the literature doesn’t provide clear guidelines, I want to explore how various ratios of P:PSA to inform decision making. Creating a modeling domain and generating PsA’s requires a lot of background code so I went ahead and created two different modelling domains with two different ratios of P:PsA. I’ve included the code to see what the various combinations look like: p1 &lt;- ggplot() + geom_sf(data = obs_bbox20kmSF, fill = &quot;white&quot;, color = &quot;black&quot;) + geom_sf(data = obs.10xPPsASF, aes(color = as.factor(pres)), size = 0.5) + geom_sf(data = SDak, fill = NA) + labs(color = &quot;Presence&quot;, y = &quot;10:1 P:PsA&quot;, title = &quot;All of South Dakota&quot;) + scale_color_viridis_d(option = &quot;C&quot;) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank()) + guides(color = FALSE) p2 &lt;- ggplot() + geom_sf(data = obs_bbox20kmSF, fill = &quot;white&quot;, color = &quot;black&quot;) + geom_sf(data = obs.100xPPsASF, aes(color = as.factor(pres)), size = 0.5) + geom_sf(data = SDak, fill = NA) + labs(color = &quot;Presence&quot;, y = &quot;100:1 P:PsA&quot;) + scale_color_viridis_d(option = &quot;C&quot;) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank()) + guides(color = FALSE) p3 &lt;- ggplot() + geom_sf(data = Eobs_bbox20kmSF, fill = &quot;white&quot;, color = &quot;black&quot;) + geom_sf(data = Eobs.10xPPsASF, aes(color = as.factor(pres)), size = 0.5) + geom_sf(data = SDak, fill = NA) + labs(color = &quot;Presence&quot;, title = &quot;Eastern South Dakota Only&quot;) + scale_color_viridis_d(option = &quot;C&quot;) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank()) p4 &lt;- ggplot() + geom_sf(data = Eobs_bbox20kmSF, fill = &quot;white&quot;, color = &quot;black&quot;) + geom_sf(data = Eobs.100xPPsASF, aes(color = as.factor(pres)), size = 0.5) + geom_sf(data = SDak, fill = NA) + labs(color = &quot;Presence&quot;) + scale_color_viridis_d(option = &quot;C&quot;) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank()) library(patchwork) (p1 | p3) / (p2 | p4) Now that I have my 4 datasets I am interested in how presences and pseudo-absences relate to various climate variables. In order to visualize these relationships, I had to extract values from climate rasters and match them to each presence and pseudo-absence location. 2.5 Climate data exploration WorldClim provides data for various temperature and precipitation measures and elevation. Let’s take a look at how some of the climate predictors differ between ratios of P:PsAs and modeling domains. But, rather than write code to makes plots for each of the four dataframes similar to how I did so above, this time I will write a function for boxplot comparisons across dataframes. plots &lt;- function(variable, y_label) { data_list &lt;- list( PPsA10x = PPsA10x_climate, PPsA100x = PPsA100x_climate, PPsA10xE = PPsA10xE_climate, PPsA100xE = PPsA100xE_climate) title_list &lt;- c(&quot;10x PPsA&quot;, &quot;100x PPsA&quot;, &quot;10x PPsA East&quot;, &quot;100x PPsA East&quot;) plot_list &lt;- lapply(seq_along(data_list), function(i) { ggplot(data_list[[i]], aes(x = factor(pres), y = .data[[variable]])) + geom_boxplot() + labs(x = &quot;Absence | Presence&quot;, y = ifelse(i == 1, y_label, &quot;&quot;)) + ggtitle(title_list[i]) + theme_bw() }) cowplot::plot_grid(plotlist = plot_list, nrow = 1) } Now I’ll use my function called “plots” to make comparisons across dataframes with various climate variables. First, let’s look at minimum temperature of the coldest month plots(variable = &quot;wc2.1_2.5m_bio_11&quot;, &quot;Temp Coldest Mo.&quot;) From the graph we can see that the average coldest temperature for presence locations is slightly colder than the average coldest temperature of pseudo-absences. But, when we consider only eastern South Dakota we see that the relationship switches. Now, the average coldest temperature of presence locations is slightly warmer than the average coldest temperature of pseudo-absence locations. Let’s look at annual precipitation plots(variable = &quot;wc2.1_2.5m_bio_12&quot;, &quot;Annual Precip&quot; ) Here we can see that when considering only eastern South Dakota, the average precipitation values for both presences and absences are greater than the average precipitation values of the entire state. This makes sense because eastern South Dakota generally receives much more rain than the west. Finally, let’s take a look at elevation plots(variable = &quot;wc2.1_2.5m_elev&quot;, &quot;Elevation&quot;) Here we see a similar pattern, When considering all of South Dakota, we notice that the mean elevation for presences is slightly lower than pseudo-absence locations, but the mean value for both presences and pseudo-absences is about the same when we restrict locations to eastern South Dakota. For further exploratory purposes, let’s standardize the values of all 20 climate and elevation predictors and plot them together to see differences. I’ll use the 10:1 P:PsA for all of South Dakota. data.frame(cbind(PPsA10x_climate[, 3, drop = F], scale(PPsA10x_climate[ , 5:24]))) %&gt;% pivot_longer(cols = starts_with(&quot;wc&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = factor(pres), y = value, color = variable)) + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 90), plot.title = element_text(size = 20)) + labs(title = &quot;&quot;) + ylab(&quot;Scaled Values&quot;) + xlab(&quot;&quot;) + geom_boxplot() + facet_wrap(~variable, scale = &quot;free&quot;) We can see that some variables have greater variation and greater differences between presences and pseudo-locations. Twenty variables is a lot, and some of these variables are likely correlated. We’ll want to take a look at the correlations between variables and remove correlated variables that can confound relationships between the response and predictor variables. Ideally, I want to aim to retain between 5-10 climate variables. 2.6 Correlations between predictor variables Let’s begin with the 10:1 P:PPsA for all of South Dakota library(corrplot) ## corrplot 0.92 loaded vars &lt;- PPsA10x_climate %&gt;% select(6:25) %&gt;% cor() %&gt;% corrplot(method = &quot;color&quot;,order = &quot;AOE&quot;, diag = FALSE, type=&quot;lower&quot;, tl.col = &quot;black&quot;, addCoef.col=&quot;black&quot;, number.cex=0.50, tl.srt = 25, main = &quot;All Variables&quot;, mar=c(0,0,2,0)) We can see that quite a few of the climate variables are highly correlated. To help me consider which predictor variables to retain from a set of highly correlated predictors, I want to evaluate the predictive power of individual predictor variables as available in sequence within the dataframe, and then graph them to make easier comparisons. ggplot(dev.fit, aes(x = x.labs, y = X2)) + geom_bar(aes(fill = x.labs), stat = &quot;identity&quot;, show.legend = FALSE) + geom_hline(yintercept = mean(dev.fit[, 2])) + theme(axis.text.x = element_text(angle = 90)) + xlab(&quot;Predictor Variable&quot;) + ylab(&quot;adj.D2&quot;) We can see from the graph that some climate variable has much greater predictive power than others. I’ll keep this in mine as I select which variables to retain in subsequent analyses. 2.7 Next steps My next steps are to assess for correlations and predictor importance for the remaining three datasets. I’m leaning towards using the full state of South Dakota for analyses purposes because there is greater variation between presence and pseudo-absence locations which may better elucidate the differences between where spotted skunks are more and less likely to occur. However, the choice of ratio of P:PsAs to use was a little less clear. Therefore, I will analyze each data set and use model accuracy metrics to determine the best choice. Stay tuned! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
